{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"tocheading\">Table of Contents</h1>\n",
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')\n// https://github.com/kmahelona/ipython_notebook_goodies\n",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')\n",
    "// https://github.com/kmahelona/ipython_notebook_goodies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import success!\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import TrackingAnalysis as TA\n",
    "#import TrackingFigures as TF\n",
    "import Utils\n",
    "\n",
    "import collections\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import functools\n",
    "import pymongo\n",
    "from pprint import pprint\n",
    "import itertools\n",
    "\n",
    "import tldextract\n",
    "from IPy import IP\n",
    "import csv\n",
    "import time\n",
    "import json\n",
    "import operator\n",
    "import copy\n",
    "import re\n",
    "import datetime\n",
    "from requests.cookies import RequestsCookieJar\n",
    "import cookies\n",
    "from http.cookiejar import Cookie\n",
    "import http.cookies\n",
    "from urllib.parse import urlparse\n",
    "import urllib.request\n",
    "from tabulate import tabulate\n",
    "from cookies import Cookie as ThirdPartyCookie\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "import seaborn\n",
    "seaborn.set_style('whitegrid')\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Import success!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mongoHost = 'localhost'\n",
    "mongoPort = 27017 #this is the default mongo port"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Graph parameters\n",
    "xtickSize = 14\n",
    "ytickSize = 14\n",
    "ylabelSize = 18\n",
    "xlabelSize = 18\n",
    "titleSize = 22\n",
    "legendSize = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "usenixRunNames = {\n",
    "    '1996': 'run:1/30/2016, 3:58:09 PM',\n",
    "    '1997': 'run:1/30/2016, 4:59:48 PM',\n",
    "    '1998': 'run:1/30/2016, 7:57:29 PM',\n",
    "    '1999': 'run:2/1/2016, 2:57:51 PM',\n",
    "    '2000': 'run:2/8/2016, 2:51:29 PM',\n",
    "    '2001': 'run:2/7/2016, 10:57:46 PM',\n",
    "    '2002': 'run:2/7/2016, 4:38:06 PM',\n",
    "    '2003': 'run:1/25/2016, 7:06:03 PM',\n",
    "    '2004': 'run:1/27/2016, 7:28:18 PM',\n",
    "    '2005': 'run:1/29/2016, 6:20:15 PM',\n",
    "    '2006': 'run:2/8/2016, 10:33:16 AM',\n",
    "    '2007': 'run:1/27/2016, 3:45:28 PM',\n",
    "    '2008': 'run:1/28/2016, 8:42:25 PM',\n",
    "    '2009': 'run:1/26/2016, 5:10:40 PM',\n",
    "    '2010': 'run:1/23/2016, 7:21:29 AM',\n",
    "    '2011': 'run:1/22/2016, 8:53:47 PM',\n",
    "    '2012': 'run:1/22/2016, 10:23:58 AM',\n",
    "    '2013': 'run:1/21/2016, 11:51:32 PM',\n",
    "    '2014': 'run:1/21/2016, 1:21:36 PM',\n",
    "    '2015': 'run:2/8/2016, 11:07:13 PM',\n",
    "    '2016': 'run:2/8/2016, 7:34:56 PM',\n",
    "}\n",
    "runNames = usenixRunNames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "@Utils.timefunc\n",
    "def reviveEngine(filename):\n",
    "    import pickle\n",
    "    with open(filename, 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reviveEngine took 0.27465367317199707 time\n",
      "reviveEngine took 1.1509466171264648 time\n",
      "reviveEngine took 1.3757250308990479 time\n",
      "reviveEngine took 1.3781156539916992 time\n",
      "reviveEngine took 2.009129524230957 time\n",
      "reviveEngine took 2.3830626010894775 time\n",
      "reviveEngine took 2.7427568435668945 time\n",
      "reviveEngine took 7.810538053512573 time\n",
      "reviveEngine took 4.985041618347168 time\n",
      "reviveEngine took 4.34147834777832 time\n",
      "reviveEngine took 6.900998592376709 time\n"
     ]
    }
   ],
   "source": [
    "def reviveWaybackEngines():\n",
    "    import pickle\n",
    "    import time\n",
    "    \n",
    "    engines = {}\n",
    "    for year in range(1996, 2017):\n",
    "        year = str(year)\n",
    "        engines[year] = reviveEngine('/app/Data/waybackCollectedData/usenix%s.pickle' % year)  #set this to be your pickle location\n",
    "        engines[year].year = year\n",
    "    return engines\n",
    "engines = reviveWaybackEngines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "liveEngines = {\n",
    "#     'Live2015Sep': reviveEngine('enginePickles/Live2015Sep.pickle'),\n",
    "#     'Live2016Feb': reviveEngine('enginePickles/Live2016Feb.pickle'),\n",
    "}    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graphs from the USENIX Paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Well Are Trackers from 2016 Archived? (Figure 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@functools.lru_cache(maxsize=1)\n",
    "def colorsOfLiveTrackers(liveEngine, wbEngine):\n",
    "    def trackersByPopularity(engine, typeFilter):\n",
    "        instances = collections.Counter()\n",
    "        for site in engine.getAllSitesWithTrackers():\n",
    "            for tracker, types in engine.sitesToTrackersToTypes[site].items():\n",
    "                if set(typeFilter) & types:  # Non-empty intersection of filter and types\n",
    "                    instances[tracker] += 1\n",
    "        return instances\n",
    "    sortedTrackers = []\n",
    "    sortedColors = []\n",
    "    greens = TA.getRequestButNoCookies(liveEngine, wbEngine)\n",
    "    reds = TA.getNoEvidence(liveEngine, wbEngine)\n",
    "    smileys = TA.getSmileys(liveEngine, wbEngine)\n",
    "    onlyInWB = TA.getOnlyInWayback(liveEngine, wbEngine)\n",
    "    for tracker, instances in trackersByPopularity(liveEngine, 'BE').most_common():\n",
    "        sortedTrackers.append(tracker)\n",
    "        if tracker in smileys:\n",
    "            sortedColors.append('Blue')\n",
    "        elif tracker in greens:\n",
    "            sortedColors.append('Green')\n",
    "        elif tracker in reds:\n",
    "            sortedColors.append('Red')\n",
    "        elif tracker in onlyInWB:\n",
    "            sortedColors.append('Purple')\n",
    "        else:\n",
    "            print('Confused about %s' % tracker)\n",
    "            sortedColors.append('Confused')\n",
    "            \n",
    "    return sortedTrackers, sortedColors\n",
    "\n",
    "def plotColorsLiveTrackers(liveEngine, wbEngine):\n",
    "\n",
    "\n",
    "    colorToNumber = {\n",
    "        'Blue': 0,\n",
    "        'Green': 1,\n",
    "        'Red': 2,\n",
    "        'Confused': 3\n",
    "    }\n",
    "    sortedTrackers, sortedColors = colorsOfLiveTrackers(liveEngine, wbEngine)\n",
    "    sortedColorNumbers = [colorToNumber[color] for color in sortedColors]\n",
    "    \n",
    "    # Try filtering for insignificance\n",
    "    nonInsignificantTrackers = []\n",
    "    nonInsignificantColors = []\n",
    "    for (tracker, color) in zip(sortedTrackers, sortedColors):\n",
    "        sitesForTrackerLive = liveEngine.getNumSitesWithTracker(tracker)\n",
    "        sitesForTrackerWB = wbEngine.getNumSitesWithTracker(tracker)\n",
    "        if sitesForTrackerLive != 1:\n",
    "            nonInsignificantTrackers.append(tracker)\n",
    "            nonInsignificantColors.append(color)\n",
    "    \n",
    "    def plotColors(colors, ax):\n",
    "    \n",
    "        fractionBlue = colors.count('Blue') / len(colors)\n",
    "        fractionGreen = colors.count('Green') / len(colors)\n",
    "        fractionRed = colors.count('Red') / len(colors)\n",
    "    #     fractionPurple = colors.count('Purple') / len(colors)\n",
    "        fractionConfused = colors.count('Confused') / len(colors)\n",
    "        data = [fractionBlue, fractionGreen, fractionRed, fractionConfused]\n",
    "\n",
    "        print(sum(data[0:3]))\n",
    "        indices = np.arange(len(data))\n",
    "        width = 1\n",
    "        bars = ax.bar(indices, data, width)\n",
    "\n",
    "\n",
    "        def autolabel(rects):\n",
    "            for rect in rects:\n",
    "                height = rect.get_height()\n",
    "#                 print(height)\n",
    "#                 print(rect.get_y())\n",
    "                plt.text(rect.get_x()+rect.get_width()/2., rect.get_y()+1.05*height, '%d%%'%float(100*height),\n",
    "                        ha='center', va='bottom', fontsize=14, fontweight='bold')\n",
    "\n",
    "\n",
    "        bars[3].set_color('#a6611a')\n",
    "        bars[2].set_color('#dfc27d')\n",
    "        bars[1].set_color('#80cdc1')\n",
    "\n",
    "        autolabel(bars)\n",
    "    #     bars[3].set_color('purple')\n",
    "        bars[0].set_color('#018571') \n",
    "        ax.set_xticks(indices+width/2)\n",
    "        ax.set_xticklabels(['Live Tracker Also\\nConfirmed In\\nArchive', \n",
    "                            'Archive Sees\\nTracker Only As\\nThird Party', \n",
    "                            'No Third Party\\nRequests For\\nTracker', \n",
    "                            'Other'], ha='center', fontsize=TF.xtickSize, fontweight='bold')\n",
    "        ax.set_title('How Well Are Trackers From 2016 Archived?', fontsize=TF.titleSize, fontweight='bold')\n",
    "        ax.set_ylabel('Fraction of 2016\\nLive Trackers', fontsize=TF.ylabelSize, fontweight='bold')\n",
    "        ax.set_yticks(np.arange(0.0, 0.55, 0.05))\n",
    "        ax.set_yticklabels(np.arange(0.0, 0.55, 0.05), fontsize=TF.ytickSize, fontweight='bold')\n",
    "\n",
    "        ax.set_ylim(0, 0.55)\n",
    "    #     indices = np.arange(len(sortedColors))\n",
    "    #     ax.scatter(indices, sortedColorNumbers)\n",
    "\n",
    "\n",
    "        #plt.savefig('figures/blueGreenRedFractionLiveWB.png', bbox_inches='tight')\n",
    "\n",
    "    fig, axes = plt.subplots()\n",
    "    fig.set_figwidth(10)\n",
    "    plotColors(sortedColors, axes)\n",
    "#     plotColors(nonInsignificantColors, axes[1])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "       \n",
    "plotColorsLiveTrackers(liveEngines['Live2016'], engines['2016'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Causes of Archival Failures (Table 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def robotsExclusions(wbEngine):\n",
    "    counts, reqs, ratios = TA.robotsRequestsRatios(wbEngine)\n",
    "\n",
    "    totalReqs = sum(reqs.values())\n",
    "    totalDomains = len(set([TA.domainOf(e.get('request_url')) for e in wbEngine.processedRequestEvents]))\n",
    "    totalURLs = len(set([e.get('request_url') for e in wbEngine.processedRequestEvents]))\n",
    "    completeRobotsDomains = [d for d in ratios if ratios[d] == 100]\n",
    "\n",
    "    \n",
    "    robotExceptionURLs = [e['request_url'] for e in wbEngine.processedRequestEvents if TA.requestFailedDueToRobotAccessControlException(e)]\n",
    "    print('%d requests blocked by robots' % len(robotExceptionURLs))\n",
    "    print('%d distinct URLs blocked by robots' % len(set(robotExceptionURLs)))\n",
    "\n",
    "    robotExceptionDomains = set([TA.domainOf(url) for url in robotExceptionURLs])\n",
    "    print('%d distinct domains blocked by robots' % len(set(robotExceptionDomains)))\n",
    "    print('%d/%d (%.1f%%) domains robots always' %\n",
    "          (len(completeRobotsDomains), totalDomains, 100*len(completeRobotsDomains)/totalDomains))\n",
    "\n",
    "def notArchived(wbEngine):\n",
    "    counts, reqs, ratios = TA.requests404sRatios(wbEngine)\n",
    "\n",
    "    totalReqs = sum(reqs.values())\n",
    "    totalDomains = len(set([TA.domainOf(e.get('request_url')) for e in wbEngine.processedRequestEvents]))\n",
    "    totalURLs = len(set([e.get('request_url') for e in wbEngine.processedRequestEvents]))\n",
    "    \n",
    "    exception404Requests = [e['request_url'] for e in wbEngine.processedRequestEvents if e.get('response_code') == 404]\n",
    "    exception404DistinctURLs = set(exception404Requests)\n",
    "    exception404DistinctDomains = set([TA.domainOf(url) for url in exception404DistinctURLs])\n",
    "    complete404Domains = [d for d in ratios if ratios[d] == 100]\n",
    "    \n",
    "    print('%d/%d (%.1f%%) requests not archived (404 error)' % (len(exception404Requests), totalReqs, 100*len(exception404Requests)/totalReqs))\n",
    "    print('%d/%d (%.1f%%) distinct URLs not archived (404 error) at least once' % \n",
    "          (len(exception404DistinctURLs), totalURLs, 100*len(exception404DistinctURLs)/totalURLs))\n",
    "    \n",
    "    print('%d/%d (%.1f%%) domains not archived (404 error) at least once' %\n",
    "          (len(exception404DistinctDomains), totalDomains, 100*len(exception404DistinctDomains)/totalDomains))\n",
    "    print('%d/%d (%.1f%%) domains not archived (404 error) always' %\n",
    "          (len(complete404Domains), totalDomains, 100*len(complete404Domains)/totalDomains))\n",
    "\n",
    "\n",
    "def waybackEscapes(wbEngine):\n",
    "    counts, reqs, ratios = TA.bubbleRequestsRatios(wbEngine)\n",
    "    totalReqsBubbled = sum(counts.values())\n",
    "    totalReqs = sum(reqs.values())\n",
    "    totalDomains = len(set([TA.domainOf(e.get('request_url')) for e in wbEngine.processedRequestEvents]))\n",
    "    totalURLs = len(set([e.get('request_url') for e in wbEngine.processedRequestEvents]))\n",
    "    distinctBubbledDomains = len([r for r in ratios.values() if r > 0])\n",
    "    distinctBubbledURLs = len(set([e.get('request_url') for e in wbEngine.processedRequestEvents if e['blocked']]))\n",
    "    completeBubbleDomains = [d for d in ratios if ratios[d] == 100]\n",
    "    print('%d/%d (%.1f%%) requests escaped' % (totalReqsBubbled, totalReqs, 100*totalReqsBubbled/totalReqs))\n",
    "    print('%d/%d (%.1f%%) distinct URLs escaped' % (distinctBubbledURLs, totalURLs, 100*distinctBubbledURLs/totalURLs))\n",
    "    print('%d/%d (%.1f%%) domains escaped at least once' %\n",
    "          (distinctBubbledDomains, totalDomains, 100*distinctBubbledDomains/totalDomains))    \n",
    "    print('%d/%d (%.1f%%) domains escaoed always' %\n",
    "          (len(completeBubbleDomains), totalDomains, 100*len(completeBubbleDomains)/totalDomains))\n",
    "\n",
    "# NOTE: The events here are not included in the denominator!\n",
    "def inconsistentTimestamps(wbEngine):\n",
    "    counts, reqs, ratios = TA.inconsistentTimestampsRatios(wbEngine)\n",
    "    totalReqsInconsistent = sum(counts.values())\n",
    "    totalReqs = sum(reqs.values())\n",
    "    totalDomains = len(set([TA.domainOf(e.get('request_url')) for e in wbEngine.processedRequestEvents]))\n",
    "    totalURLs = len(set([e.get('request_url') for e in wbEngine.processedRequestEvents]))\n",
    "    distinctInconsistentDomains = len([r for r in ratios.values() if r > 0])\n",
    "    distinctInconsistentURLs = len(set([e.get('request_url') for e in wbEngine.processedRequestEvents if e['blocked']]))\n",
    "    completeInconsistentDomains = [d for d in ratios if ratios[d] == 100]\n",
    "    print('%d/%d (%.1f%%) requests had inconsistent timestamps' % (totalReqsInconsistent, totalReqs, 100*totalReqsInconsistent/totalReqs))\n",
    "    print('%d/%d (%.1f%%) distinct URLs had inconsistent timestamps' % (distinctInconsistentURLs, totalURLs, 100*distinctInconsistentURLs/totalURLs))\n",
    "    print('%d/%d (%.1f%%) domains had inconsistent timestamps at least once' %\n",
    "          (distinctInconsistentDomains, totalDomains, 100*distinctInconsistentDomains/totalDomains))    \n",
    "    print('%d/%d (%.1f%%) domains had inconsistent timestamps always' %\n",
    "          (len(completeInconsistentDomains), totalDomains, 100*len(completeInconsistentDomains)/totalDomains))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "robotsExclusions(engines['2016'])\n",
    "notArchived(engines['2016'])\n",
    "waybackEscapes(engines['2016'])\n",
    "inconsistentTimestamps(engines['2016'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evolution of Tracker Types Over Time (Figure 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def letterTrackingOverTime(engines, numSites, historicalDF, runNames):\n",
    "    for y, eng in engines.items(): \n",
    "        siteList = TF.getOrderedTopNSites(mongo, runNames[y], numSites)\n",
    "        setOfTrackers = defaultdict(set)\n",
    "        for s in siteList:\n",
    "            for ty in 'FEDCBA':\n",
    "                tempSet = eng.getTrackersOfTypeOnSite(s, ty)\n",
    "                setOfTrackers['total'] = setOfTrackers['total'].union(tempSet)\n",
    "                setOfTrackers[ty] = setOfTrackers[ty].union(tempSet) # this function does not double-count B/C B/E\n",
    "\n",
    "        for ty in 'ABCDEF':\n",
    "            columnName = ty+str(numSites)\n",
    "            historicalDF.at[int(y), columnName] = len(setOfTrackers[ty])\n",
    "    \n",
    "        totalCol = 'TotalTrackingDomains'+str(numSites)\n",
    "        historicalDF.at[int(y), totalCol] = len(setOfTrackers['total'])\n",
    "    historicalDF.sort(inplace=True)\n",
    "    print(historicalDF)\n",
    "    #writeHistoricalDF(historicalDF)  \n",
    "    return historicalDF\n",
    "  \n",
    "\n",
    "def graphLetterTracking(engines, numSites, historicalDF, runNames):\n",
    "    numSitesStr = str(numSites)\n",
    "    historicalDF = letterTrackingOverTime(engines, numSites, historicalDF, runNames)\n",
    "    statsList = ['A', 'B', 'C', 'D', 'E', 'F']\n",
    "    typeNames = ['Analytics', 'Vanilla', 'Forced', 'Referred', 'Personal', 'Referred Analytics']\n",
    "    colList = [stat+numSitesStr for stat in statsList]\n",
    "    dfSlice = historicalDF[historicalDF[colList[1]] >= 0][colList]\n",
    "    dfSlice.columns = typeNames\n",
    "\n",
    "    #Make a graph\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    figDim = len(dfSlice)/1.75\n",
    "    fig = plt.figure(figsize=(10, 6))\n",
    "    ax = fig.gca()\n",
    "    #plt.gca().tight_layout()\n",
    "    #plt.gcf().subplots_adjust(bottom=0.50)\n",
    "    #fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "    my_colors = [(55,126,184),(77,175,74),(228,26,28),(152,78,163),(255,127,0),(166,86,40)]\n",
    "    my_colors = [(r/255, g/255, b/255) for (r, g, b) in my_colors]\n",
    "    my_dashes = [(None, None), [5,8], [5,3,2,3], [2,3], [5,2,5,2,5,10],\n",
    "               [3,2,3,10], (None, None)]\n",
    "    for ty, color, dash in zip(dfSlice.columns, my_colors, my_dashes):\n",
    "        plt.plot(dfSlice.index, dfSlice[ty], color=color, dashes=dash, linewidth=5, label=ty)\n",
    "    width=.8\n",
    "    plt.bar(historicalDF.index-width/2,\n",
    "              historicalDF['TotalTrackingDomains'+numSitesStr],\n",
    "              width, color=(211/255, 211/255, 211/255), label='Total Tracker Domains')\n",
    "    #plt.xticks(numpy.arange(1, len(historicalDF)+1)+1/2, historicalDF.index)\n",
    "    ax.set_title('Trackers of Each Type In Dataset (Top {} Sites)'.format(numSitesStr), fontweight=\"bold\", fontsize=titleSize)\n",
    "    ax.set_ylabel('Trackers in Dataset', fontweight=\"bold\", fontsize=ylabelSize)\n",
    "    ax.set_xlabel('Year', fontweight=\"bold\", fontsize=xlabelSize)\n",
    "    ax.set_xticks(historicalDF.index)\n",
    "    ax.set_xticklabels(historicalDF.index, rotation='45', fontweight=\"bold\", fontsize=xtickSize)\n",
    "    ax.set_yticklabels(ax.get_yticks().tolist(), fontweight=\"bold\", fontsize=ytickSize)\n",
    "    plt.xlim(dfSlice.index.values[0]-.5, dfSlice.index.values[-1]+.5)\n",
    "    #ax.autoscale(axis='y', tight=True)\n",
    "    ax.legend(loc=2, fontsize=legendSize+2)\n",
    "    fig.set_tight_layout(True)\n",
    "    fig.savefig('graphs/usenix/typesOverTime.png')\n",
    "    \n",
    "graphLetterTracking(wbEngines, 500, pd.DataFrame(), wbRunNamesForDataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complexity of Trackers (Table 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def quantityOfTypesDistribution(engines, numSites, historicalDF, runNames, include3=False, live=False):\n",
    "    typeList = ['A', 'B', 'C', 'D', 'E', 'F']\n",
    "    colSuffix = 'Type_'+str(numSites)\n",
    "    if include3:\n",
    "        typeList.append('3')\n",
    "        colSuffix += '_with3'\n",
    "\n",
    "    multipleTypeCounter = defaultdict(Counter)\n",
    "    for y, eng in engines.items():\n",
    "        typesOfTracker = defaultdict(set)\n",
    "        if not live:\n",
    "            key = int(y)\n",
    "        else:\n",
    "            key = y\n",
    "        siteList = TF.getOrderedTopNSites(mongo, runNames[y], numSites)\n",
    "        # what we want is for each year how many trackers (on the subset of 450 sites had how many types)\n",
    "        # so year -> tracker -> types_seen\n",
    "        for s in siteList:\n",
    "            for t in eng.getTrackersOnSite(s):\n",
    "                tType = eng.getCombinedTypeOfTrackerOnSite(s, t).split(',')\n",
    "                for ty in tType:\n",
    "                    if ty in 'ABCDEF':\n",
    "                        typesOfTracker[t].add(ty)\n",
    "                \n",
    "        totalTrackers = len(typesOfTracker)\n",
    "        valueCounter = defaultdict(int)\n",
    "        multipleTypeCounter[key] = Counter()\n",
    "        for t, v in typesOfTracker.items():\n",
    "            vLen = len(v)\n",
    "            if 'E' in v or 'C' in v:\n",
    "                vLen -= 1 # subtract out double-counted B's\n",
    "            valueCounter[vLen] += 1\n",
    "            if vLen > 1:\n",
    "                multipleTypeCounter[key][\"\".join(v)] += 1\n",
    "        for n in range(1, 1+len(typeList)):\n",
    "            colName = str(n)+colSuffix\n",
    "            valToInput = \"0\"\n",
    "            if totalTrackers > 0:\n",
    "                num = valueCounter[n]\n",
    "            if num > 0:\n",
    "                valToInput = \"{:.2%} ({})\".format(num/totalTrackers, num)\n",
    "            historicalDF.at[key, colName] = valToInput\n",
    "    historicalDF.sort(inplace=True)\n",
    "    #print(historicalDF)\n",
    "    #print(multipleTypeCounter)\n",
    "    #tracker->TypeSet -> tracker->len(typeSet) -> len(typeSet)->len(trackerSet)\n",
    "    # just make this a table\n",
    "\n",
    "    return historicalDF, multipleTypeCounter\n",
    "\n",
    "def tableQuantityOfTypes(engines, numSites, myDF, runNames, live=False):\n",
    "    myDF, multipleTypes = quantityOfTypesDistribution(engines, numSites, myDF, runNames, include3=False, live=live)\n",
    "    #print(\"Top {}\".format(numSites))\n",
    "    #for k in sorted(multipleTypes.keys()):\n",
    "    #    if len(multipleTypes[k]) > 0:\n",
    "    #        print(\"{}: {}\".format(k, multipleTypes[k].most_common()))\n",
    "    colNames = [str(n)+'Type_'+str(numSites) for n in range(1,5)] #no3\n",
    "    if not live:\n",
    "        keyList = [int(k) for k in runNames.keys()]\n",
    "    else:\n",
    "        keyList = runNames.keys()\n",
    "    dfSlice = myDF[myDF.index.isin(keyList)]\n",
    "    dfSlice = dfSlice.loc[:, colNames]\n",
    "    #with open('graphs/usenix/typeDistribution.tex', 'w') as outfile:\n",
    "    #    outfile.write(tabulate(dfSlice, headers=['Year']+[i[0:5] for i in colNames], tablefmt=\"latex\"))\n",
    "    print(tabulate(dfSlice, stralign=\"right\", headers=['Year']+[i[0:5] for i in colNames]))\n",
    "    \n",
    "tableQuantityOfTypes(wbEngines, 500, pd.DataFrame(), wbRunNamesForDataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use of LocalStorage (Figure 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def localStorageStackedBar():\n",
    "    \n",
    "    def getAllAPIEventsForYear(api, year):\n",
    "        engine = engines[year]\n",
    "        fpEvents = list(mongo.hammer.fingerprintApiEvent.find({'run_name': engine.run_name, \n",
    "                                                               'api_name': api}))\n",
    "#         print('%d %s events in year %s' % (len(fpEvents), api, year))\n",
    "        return fpEvents\n",
    "    \n",
    "    def isFirstPartyFPEvent(event):\n",
    "        return event['frame']['parentFrameId'] == -1 and event['setting_script_domain'] == event['top_domain']\n",
    "    def isThirdPartyFPEvent(event):\n",
    "        return event['frame']['parentFrameId'] == -1 and event['setting_script_domain'] != event['top_domain']\n",
    "    def isFramedFPEvent(event):\n",
    "        return event['frame']['parentFrameId'] != -1\n",
    "    \n",
    "    def classifyFPEvent(event):\n",
    "        if isFirstPartyFPEvent(event):\n",
    "            return 'first party'\n",
    "        elif isThirdPartyFPEvent(event):\n",
    "            return 'third party'\n",
    "        elif isFramedFPEvent(event):\n",
    "            return 'framed'\n",
    "        else:\n",
    "            pprint(event)\n",
    "            raise ValueError('confused about event')\n",
    "            return 'confused'\n",
    "\n",
    "        \n",
    "    def getDataPerClassification(api):\n",
    "        dataPerYear = []\n",
    "        for year in range(1996, 2017):\n",
    "            year = str(year)\n",
    "       \n",
    "            engine = engines[year]\n",
    "            # Get all the localStorage FP events for each year.\n",
    "            fpEvents = getAllAPIEventsForYear(api, year)\n",
    "            # Filter out anachronisms.\n",
    "    #         print('%d events' % len(fpEvents))\n",
    "    #         print(engine.pastLimit, engine.futureLimit)\n",
    "            fpEvents = [event for event in fpEvents if not TA.isAnachronism(event, engine.pastLimit, engine.futureLimit)]\n",
    "    #         print('%d events after anachronism removal' % len(fpEvents))\n",
    "            # Prepare to compute the below by calculating the script_setting_domain and top_domain from those URLs.        \n",
    "            for event in fpEvents:\n",
    "                event['setting_script_domain'] = TA.domainOf(TA.getArchivedURLFromWaybackURL(event['setting_script_url']))\n",
    "                event['top_domain'] = TA.domainOf(TA.getArchivedURLFromWaybackURL(event['top_url']))\n",
    "                event['frame_domain'] = TA.domainOf(TA.getArchivedURLFromWaybackURL(event['frame']['url']))\n",
    "                event['classification'] = classifyFPEvent(event)\n",
    "\n",
    "\n",
    "#             classifications = [e['classification'] for e in fpEvents]\n",
    "#             tops = set([e['top_domain'] for e in fpEvents if e['classification'] == 'first party'])\n",
    "#             thirds = set([e['setting_script_domain'] for e in fpEvents if e['classification'] == 'third party'])\n",
    "\n",
    "#             realSites = set(TF.getOrderedTopNSites(mongo, engine.run_name, 500))\n",
    "    #         print('%d/%d thirds are real top 500 sites' % (len(thirds & realSites), len(thirds)))\n",
    "    #         print('%d/%d tops are real top 500 sites' % (len(tops & realSites), len(tops)))\n",
    "\n",
    "    #         print(tops)\n",
    "    #         allTopDomains = set([e['top_domain'] for e in fpEvents])\n",
    "    #         allScriptDomains = set([e['setting_script_domain'] for e in fpEvents])\n",
    "    #         print('tops', allTopDomains)\n",
    "    #         print('scripts', allScriptDomains)\n",
    "\n",
    "    #         print('%s: Intersection of all top and all script domains is size %d' % (year, len(allTopDomains & allScriptDomains)))\n",
    "\n",
    "    #         print(tops)\n",
    "    #         print(thirds)\n",
    "    #         print('%s: Intersection of tops and thirds is size %d' % (year, len(tops & thirds)))\n",
    "    #         print(tops & thirds)\n",
    "\n",
    "            dataPerYear.append([\n",
    "                len(set([e['top_domain'] for e in fpEvents if e['classification'] == 'first party'])), # first party\n",
    "    #             classifications.count('first party'),\n",
    "                len(set([e['setting_script_domain'] for e in fpEvents if e['classification'] == 'third party'])), # third party\n",
    "    #             classifications.count('third party'),\n",
    "    #             classifications.count('framed')\n",
    "                len(set([e['frame_domain'] for e in fpEvents if e['classification'] == 'framed'])) # third party\n",
    "            ])\n",
    "        dataPerClassification = list(zip(*dataPerYear))\n",
    "        dataPerClassification = ([np.array(row) for row in dataPerClassification])\n",
    "        return dataPerClassification\n",
    "\n",
    "\n",
    "    localStorageDataPerClassification = getDataPerClassification('window.localStorage')\n",
    "#     doNotTrackDataPerClassification = getDataPerClassification('navigator.doNotTrack')\n",
    "    print(localStorageDataPerClassification)\n",
    "#     print(doNotTrackDataPerClassification)\n",
    "    \n",
    "#     dataPerClassification = doNotTrackDataPerClassification\n",
    "#     bottoms = np.zeros(len(dataPerClassification[0]))\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(localStorageDataPerClassification[0], marker='o')\n",
    "    ax.plot(localStorageDataPerClassification[1], color='orange', marker='o')\n",
    "    ax.plot(localStorageDataPerClassification[2], color='red', marker='o')\n",
    "    \n",
    "#     ax[1].plot(doNotTrackDataPerClassification[0], marker='*')\n",
    "#     ax[1].plot(doNotTrackDataPerClassification[1], color='orange', marker='*')\n",
    "#     ax[1].plot(doNotTrackDataPerClassification[2], color='red', marker='*')\n",
    "#     indices = range(len(dataPerClassification[0]))\n",
    "#     ax.bar(indices, dataPerClassification[0])\n",
    "#     bottoms += dataPerClassification[0]\n",
    "#     ax.bar(indices, dataPerClassification[1], bottom=bottoms, color='orange')\n",
    "#     bottoms += dataPerClassification[1]\n",
    "#     ax.bar(indices, dataPerClassification[2], bottom=bottoms, color='red')\n",
    "        \n",
    "    ax.legend([\n",
    "        'First party localStorage',\n",
    "        'Third party localStorage',\n",
    "        'Framed localStorage',\n",
    "    ], loc='upper left', fontsize=12)\n",
    "#     ax[1].legend([\n",
    "#         'First party doNotTrack',\n",
    "#         'Third party doNotTrack',\n",
    "#         'Framed doNotTrack'\n",
    "#     ], loc='upper left')\n",
    "    \n",
    "#     ax.set_xticklabels([])\n",
    "    ax.set_xticklabels(range(1996, 2017, 5))\n",
    "\n",
    "    ax.get_yaxis().set_label_coords(-0.075, 0.5)\n",
    "    ax.set_ylabel('Domains', fontsize=TF.ylabelSize-2)\n",
    "    ax.set_title('Use of localStorage', fontsize=TF.titleSize-4, fontweight='bold')\n",
    "    ax.get_yaxis().set_label_coords(-0.025, 0.5)\n",
    "    ax.get_yaxis().tick_right()\n",
    "\n",
    "    fig.set_figheight(1.5)\n",
    "    #fig.savefig('figures/localStorageAndDNT.png', bbox_inches='tight')\n",
    "    \n",
    "localStorageStackedBar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution of Third Parties Requested Per Site (Figure 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def boxThirdsPerYear(numSites, engines):\n",
    "    years = []\n",
    "    numReqs = []\n",
    "    for y, eng in engines.items():\n",
    "        siteList = TF.getOrderedTopNSites(mongo, wbRunNamesForDataset[y], numSites)\n",
    "        for s in siteList:\n",
    "            years.append(int(y))\n",
    "            numReqs.append(eng.getNumTrackersOfTypeOnSite(s, '3'))\n",
    "    \n",
    "    #print(len(years))\n",
    "    thirdsDF = pd.DataFrame({'year': years, 'numThirdPartyReqs': numReqs})\n",
    "                        \n",
    "   # print(df)\n",
    "    thirdsDF = thirdsDF.sort_values('year')\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    fig = plt.figure(figsize=(12, 6))\n",
    "    ax = fig.gca()\n",
    "    ax = sns.boxplot(data=thirdsDF, x='year', y='numThirdPartyReqs', ax=ax, color=\"#deebf7\", \n",
    "                     flierprops={'alpha':0.6,'markersize': 10, 'marker': '.'}, \n",
    "                    boxprops = dict(linewidth=3, color='black'), \n",
    "                    whiskerprops = dict(linewidth=3, color='black'),\n",
    "                    capprops = dict(linewidth=3, color='black'),\n",
    "                    medianprops = dict(linewidth=3, color='black'))\n",
    "    plt.title('Third Parties Requested Per Site (Top {} Sites)'.format(numSites), fontweight=\"bold\", fontsize=titleSize)\n",
    "    plt.ylabel('Third Parties Requested per Site', fontweight=\"bold\", fontsize=ylabelSize)\n",
    "    plt.xlabel('Year', fontweight=\"bold\", fontsize=xlabelSize)\n",
    "    plt.xticks(fontweight=\"bold\", fontsize=xtickSize, rotation='45', ha='right')\n",
    "    plt.yticks(fontweight=\"bold\", fontsize=ytickSize)\n",
    "    #fig.savefig('graphs/usenix/boxplotThirds'+str(numSites)+'.png')\n",
    "    plt.show()\n",
    "    \n",
    "boxThirdsPerYear(500, wbEngines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution of Third-Party Requests (Figure 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def thirdCDF(engines, numSites, historicalDF, runNames):\n",
    "    # eventually want a number of trackers (0-max ever seen+1)-> % of domains per year \n",
    "    # (% of domains just means dividing by total # of domains)\n",
    "    # year is still index. Columns for each number of trackers? Yikes.\n",
    "    beforePercentsTable = defaultdict(list)\n",
    "    maxNumDomainsOnSite = 0\n",
    "    for y, eng in engines.items():\n",
    "        siteList = TF.getOrderedTopNSites(mongo, runNames[y], numSites)\n",
    "        thirdReqs = Counter()\n",
    "        for s in siteList:\n",
    "            thirdReqs[s] = eng.getNumTrackersOfTypeOnSite(s, '3')\n",
    "        # now I have a counter with the number of thirdReqs for each site that year\n",
    "        # counter most_common gives me the largest count\n",
    "        key = int(y)\n",
    "        totalSites = len(siteList)\n",
    "        mostCommonCount = thirdReqs.most_common(1)[0][1]\n",
    "        maxNumDomainsOnSite = max(maxNumDomainsOnSite, mostCommonCount)\n",
    "        #print(mostCommonCount)\n",
    "        howManyDomainsWithThirdCount = defaultdict(int)\n",
    "        for (s, numThirdReqs) in thirdReqs.most_common():\n",
    "            howManyDomainsWithThirdCount[numThirdReqs] += 1\n",
    "        #print(\"{} {}\".format(key, howManyDomainsWithThirdCount))\n",
    "        # so now what I have is the count of how many domains have each tracker number\n",
    "        # to get how many have at least each tracker number add the sum of everything greater\n",
    "        howManyDomainsWithAtLeastThirdCount = defaultdict(int)\n",
    "        for n in reversed(range(1+mostCommonCount)):\n",
    "            howManyDomainsWithAtLeastThirdCount[n] = howManyDomainsWithThirdCount[n] + howManyDomainsWithAtLeastThirdCount[n+1]\n",
    "        beforePercentsTable[key] = [(n, v) for n, v in howManyDomainsWithAtLeastThirdCount.items()]\n",
    "        #historicalDF.at[key, \"AtLeast0Capable_\"+str(numSites)] = 100.0\n",
    "        for thirdCount, numDomains in howManyDomainsWithAtLeastThirdCount.items():\n",
    "            historicalDF.at[key, \"AtLeast{}Capable_{}\".format(thirdCount, numSites)] = 100*numDomains/totalSites\n",
    "        historicalDF.at[key, \"AtLeast0Capable_\"+str(numSites)] = 100.0\n",
    "        historicalDF.at[key, \"AtLeast{}Capable_{}\".format(mostCommonCount+1, numSites)] = 0.0\n",
    "    beforePercentsTableItems = beforePercentsTable.items()\n",
    "    rawDF = pd.DataFrame({'Year': [k for k,_ in beforePercentsTableItems],\n",
    "                        'NumTracker, HowManyDomainsHaveAtLeastNumTrackerTrackers': [v for _,v in beforePercentsTableItems]\n",
    "                       })\n",
    "    #print(rawDF)\n",
    "    #rawDF.to_csv('graphs/rawReverseCDF_top{}.csv'.format(numSites))\n",
    "    historicalDF.sort(inplace=True)\n",
    "    #writeHistoricalDF(historicalDF)\n",
    "    return historicalDF, maxNumDomainsOnSite+1\n",
    "\n",
    "def graphThirdCDF(engines, numSites, historicalDF, runNames, showEven=False):\n",
    "    historicalDF, maxColNumber = thirdCDF(engines, numSites, historicalDF, runNames)\n",
    "    print(\"Max col number: {}\".format(maxColNumber))\n",
    "    columnsToSlice = [\"AtLeast{}Capable_{}\".format(i, numSites) for i in range(1+maxColNumber)]\n",
    "    dfSlice = historicalDF[historicalDF[columnsToSlice[0]] > 0][columnsToSlice]\n",
    "    dfSlice.columns = range(1+maxColNumber)\n",
    "    #print(dfSlice.columns)\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    #figDim = len(dfSlice)/1.5\n",
    "    fig = plt.figure(figsize=(11, 5))\n",
    "    ax = fig.gca()\n",
    "    plt.title('Distribution of Third-Party Requests (Top {} Sites)'.format(numSites), fontweight=\"bold\", fontsize=titleSize)\n",
    "    plt.ylabel('Percentage of Sites', fontweight=\"bold\", fontsize=ylabelSize)\n",
    "    plt.xlabel('Minimum Number of Third-Parties', fontweight=\"bold\", fontsize=xlabelSize)\n",
    "    if showEven:\n",
    "        howManyYears = len([y for y in dfSlice.index.values if y % 2 != 1])\n",
    "    else:\n",
    "        howManyYears = len(dfSlice.index.values)\n",
    "    my_colors = sns.cubehelix_palette(howManyYears, light=.7, rot=-1.2, reverse=True)\n",
    "    for year, i in zip(dfSlice.index.values, range(2*len(my_colors))):\n",
    "        if not showEven:\n",
    "            dfSlice2 = dfSlice.loc[year, :]\n",
    "            dfSlice2.plot(kind='line', color=my_colors[i], label=year, linewidth=4)\n",
    "        elif year % 2 != 1:\n",
    "            dfSlice2 = dfSlice.loc[year, :]\n",
    "            dfSlice2.plot(kind='line', color=my_colors[int(math.floor(i/2))],\n",
    "                        label=year, linewidth=4)\n",
    "    plt.xticks(fontweight=\"bold\", fontsize=xtickSize) #rotation='45', ha='right')\n",
    "    plt.yticks(fontweight=\"bold\", fontsize=ytickSize)  \n",
    "    ax.legend(loc='upper right', fontsize=legendSize)\n",
    "    plt.minorticks_on()\n",
    "    plt.grid(b=True, which='minor')\n",
    "    fig.set_tight_layout(True)\n",
    "    #fig.savefig('graphs/usenix/thirdPartiesCDF'+str(numSites)+'.png')\n",
    "    plt.show()\n",
    "\n",
    "graphThirdCDF(wbEngines, 500, pd.DataFrame(), wbRunNamesForDataset, showEven=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coverage Evolution Over Time (Figure 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Utility functions for coverage\n",
    "\n",
    "def percentSitesContaining(sortedEngineYears, sortedEngines, typeFilter):\n",
    "    def trackersByPopularity(engine, typeFilter):\n",
    "        instances = collections.defaultdict(set)\n",
    "        for site in engine.getAllSitesWithTrackers():\n",
    "            for tracker, types in engine.sitesToTrackersToTypes[site].items():\n",
    "                if set(typeFilter) & types:  # Non-empty intersection of filter and types\n",
    "                    instances[tracker].add(site)\n",
    "        return instances\n",
    "    \n",
    "    def mostCommon(dictOfSets, N):        \n",
    "        return sorted(((tracker, sites) for tracker, sites in dictOfSets.items()), reverse=True, key=lambda x: len(x[1]))[:N]\n",
    "\n",
    "    def ratiosForEngine(engine):\n",
    "        instances = trackersByPopularity(engine, typeFilter)\n",
    "#         total = len(engine.getAllSitesWithTrackers())\n",
    "        total = len(engine.getAllSites())\n",
    "        #print('There are %d sites with trackers' % total)\n",
    "        try:\n",
    "            top1Sites = set.union(*[t[1] for t in mostCommon(instances, 1)])\n",
    "            top1Count = len(top1Sites)\n",
    "            top5Sites = set.union(*[t[1] for t in mostCommon(instances, 5)])\n",
    "            top5Count = len(top5Sites)\n",
    "            top10Sites = set.union(*[t[1] for t in mostCommon(instances, 10)])\n",
    "            top10Count = len(top10Sites)\n",
    "            top20Sites = set.union(*[t[1] for t in mostCommon(instances, 20)])\n",
    "            top20Count = len(top20Sites)\n",
    "            \n",
    "#             print('Top 20 in %s: %s' % (engine.year, top20Sites))\n",
    "       \n",
    "            top1Ratio = 100*top1Count/total\n",
    "            top5Ratio = 100*top5Count/total\n",
    "            top10Ratio = 100*top10Count/total\n",
    "            top20Ratio = 100*top20Count/total\n",
    "        except: \n",
    "            print('Year %s has total 0 tracking' % year)\n",
    "            top1Ratio = 0\n",
    "            top5Ratio = 0\n",
    "            top10Ratio = 0\n",
    "            top20Ratio = 0\n",
    "        \n",
    "        return top1Ratio, top5Ratio, top10Ratio, top20Ratio\n",
    "\n",
    "    data = []\n",
    "    for year, engine in zip(sortedEngineYears, sortedEngines):\n",
    "        top1Ratio, top5Ratio, top10Ratio, top20Ratio = ratiosForEngine(engine)\n",
    "        data.append([year, top1Ratio, top5Ratio, top10Ratio, top20Ratio])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getColors():\n",
    "    colors = [\n",
    "        '#fbb4b9',\n",
    "        '#f768a1',\n",
    "        '#c51b8a',\n",
    "        '#7a0177'\n",
    "    ]\n",
    "    for color in colors:\n",
    "        yield color\n",
    "        \n",
    "def linePlot(data, ax, typeFilter, legendLocation, yLabel=True):\n",
    "    import matplotlib.ticker as mtick\n",
    "    years, top1s, top5s, top10s, top20s = zip(*data)\n",
    "    topData = [top20s, top10s, top5s, top1s]\n",
    "    for d, color in zip(topData, getColors()):\n",
    "        ax.plot(d, color=color)\n",
    "#     ax.bar(range(len(years), len(years)+4), \n",
    "#            [live1Ratio, live5Ratio, live10Ratio, live20Ratio],\n",
    "#           color='yellow')\n",
    "\n",
    "    ax.legend( #(line1, line5, line10, line20),\n",
    "                ('Top 20 Trackers', 'Top 10 Trackers', 'Top 5 Trackers', 'Top 1 Tracker'), \n",
    "                loc=legendLocation, fontsize=legendSize)\n",
    "# Utility functions for coverage\n",
    "\n",
    "def percentSitesContaining(sortedEngineYears, sortedEngines, typeFilter):\n",
    "    def trackersByPopularity(engine, typeFilter):\n",
    "        instances = collections.defaultdict(set)\n",
    "        for site in engine.getAllSitesWithTrackers():\n",
    "            for tracker, types in engine.sitesToTrackersToTypes[site].items():\n",
    "                if set(typeFilter) & types:  # Non-empty intersection of filter and types\n",
    "                    instances[tracker].add(site)\n",
    "        return instances\n",
    "    \n",
    "    def mostCommon(dictOfSets, N):        \n",
    "        return sorted(((tracker, sites) for tracker, sites in dictOfSets.items()), reverse=True, key=lambda x: len(x[1]))[:N]\n",
    "\n",
    "    def ratiosForEngine(engine):\n",
    "        instances = trackersByPopularity(engine, typeFilter)\n",
    "#         total = len(engine.getAllSitesWithTrackers())\n",
    "        total = len(engine.getAllSites())\n",
    "        #print('There are %d sites with trackers' % total)\n",
    "        try:\n",
    "            top1Sites = set.union(*[t[1] for t in mostCommon(instances, 1)])\n",
    "            top1Count = len(top1Sites)\n",
    "            top5Sites = set.union(*[t[1] for t in mostCommon(instances, 5)])\n",
    "            top5Count = len(top5Sites)\n",
    "            top10Sites = set.union(*[t[1] for t i# Utility functions for coverage\n",
    "\n",
    "def percentSitesContaining(sortedEngineYears, sortedEngines, typeFilter):\n",
    "    def trackersByPopularity(engine, typeFilter):\n",
    "        instances = collections.defaultdict(set)\n",
    "        for site in engine.getAllSitesWithTrackers():\n",
    "            for tracker, types in engine.sitesToTrackersToTypes[site].items():\n",
    "                if set(typeFilter) & types:  # Non-empty intersection of filter and types\n",
    "                    instances[tracker].add(site)\n",
    "        return instances\n",
    "    \n",
    "    def mostCommon(dictOfSets, N):        \n",
    "        return sorted(((tracker, sites) for tracker, sites in dictOfSets.items()), reverse=True, key=lambda x: len(x[1]))[:N]\n",
    "\n",
    "    def ratiosForEngine(engine):\n",
    "        instances = trackersByPopularity(engine, typeFilter)\n",
    "#         total = len(engine.getAllSitesWithTrackers())\n",
    "        total = len(engine.getAllSites())\n",
    "        #print('There are %d sites with trackers' % total)\n",
    "        try:\n",
    "            top1Sites = set.union(*[t[1] for t in mostCommon(instances, 1)])\n",
    "            top1Count = len(top1Sites)\n",
    "            top5Sites = set.union(*[t[1] for t in mostCommon(instances, 5)])\n",
    "            top5Count = len(top5Sites)\n",
    "            top10Sites = set.union(*[t[1] for t in mostCommon(instances, 10)])\n",
    "            top10Count = len(top10Sites)\n",
    "            top20Sites = set.union(*[t[1] for t in mostCommon(instances, 20)])\n",
    "            top20Count = len(top20Sites)\n",
    "            \n",
    "#             print('Top 20 in %s: %s' % (engine.year, top20Sites))\n",
    "       \n",
    "            top1Ratio = 100*top1Count/total\n",
    "            top5Ratio = 100*top5Count/total\n",
    "            top10Ratio = 100*top10Count/total\n",
    "            top20Ratio = 100*top20Count/total\n",
    "        except: \n",
    "            print('Year %s has total 0 tracking' % year)\n",
    "            top1Ratio = 0\n",
    "            top5Ratio = 0\n",
    "            top10Ratio = 0\n",
    "            top20Ratio = 0\n",
    "        \n",
    "        return top1Ratio, top5Ratio, top10Ratio, top20Ratio\n",
    "\n",
    "    data = []\n",
    "    for year, engine in zip(sortedEngineYears, sortedEngines):\n",
    "        top1Ratio, top5Ratio, top10Ratio, top20Ratio = ratiosForEngine(engine)\n",
    "        data.append([year, top1Ratio, top5Ratio, top10Ratio, top20Ratio])\n",
    "    return datan mostCommon(instances, 10)])\n",
    "            top10Count = len(top10Sites)\n",
    "            top20Sites = set.union(*[t[1] for t in mostCommon(instances, 20)])\n",
    "            top20Count = len(top20Sites)\n",
    "            \n",
    "#             print('Top 20 in %s: %s' % (engine.year, top20Sites))\n",
    "       \n",
    "            top1Ratio = 100*top1Count/total\n",
    "            top5Ratio = 100*top5Count/total\n",
    "            top10Ratio = 100*top10Count/total\n",
    "            top20Ratio = 100*top20Count/total\n",
    "        except: \n",
    "            print('Year %s has total 0 tracking' % year)\n",
    "            top1Ratio = 0\n",
    "            top5Ratio = 0\n",
    "            top10Ratio = 0\n",
    "            top20Ratio = 0\n",
    "        \n",
    "        return top1Ratio, top5Ratio, top10Ratio, top20Ratio\n",
    "\n",
    "    data = []\n",
    "    for year, engine in zip(sortedEngineYears, sortedEngines):\n",
    "        top1Ratio, top5Ratio, top10Ratio, top20Ratio = ratiosForEngine(engine)\n",
    "        data.append([year, top1Ratio, top5Ratio, top10Ratio, top20Ratio])\n",
    "    return data\n",
    "    ax.set_title('%s Coverage' % typeFilter, y=1.04, fontsize=titleSize, fontweight='bold')\n",
    "    ax.set_xlabel('Years', fontsize=xlabelSize, fontweight='bold')\n",
    "    if yLabel:\n",
    "        ax.set_ylabel('% Sites', fontsize=ylabelSize, fontweight='bold')\n",
    "    ax.set_ylim(0, 100)\n",
    "    ax.set_xticks(range(len(years)))\n",
    "    ax.set_xticklabels(years, rotation=45, ha='right', fontsize=xtickSize, fontweight='bold')\n",
    "    ax.set_yticklabels(ax.get_yticks().tolist(), fontsize=ytickSize, fontweight='bold')\n",
    "#         ax.set_ylim(0, .7)\n",
    "\n",
    "    for label in ax.xaxis.get_ticklabels()[1::2]:\n",
    "        label.set_visible(False)\n",
    "\n",
    "    fmt = '%.0f%%' # Format you want the ticks, e.g. '40%'\n",
    "    yticks = mtick.FormatStrFormatter(fmt)\n",
    "    ax.yaxis.set_major_formatter(yticks)#set_yticks(range(0, 40, 5))\n",
    "\n",
    "def barPlot(data, ax):\n",
    "    import matplotlib.ticker as mtick\n",
    "\n",
    "    indices = np.arange(0, len(data)/2, 0.5)\n",
    "    width = 0.35\n",
    "    bars = ax.bar(indices, data, width)\n",
    "    colors = getColors()\n",
    "\n",
    "#     fmt = '%.0f%%' # Format you want the ticks, e.g. '40%'\n",
    "#     yticks = mtick.FormatStrFormatter(fmt)\n",
    "#     ax.yaxis.set_major_formatter(yticks)#set_yticks(range(0, 40, 5))\n",
    "\n",
    "    for i, color in enumerate(reversed(list(colors))):\n",
    "        bars[i].set_color(color)\n",
    "    ax.set_title('2016 Tracker Coverage', y=1.04, fontsize=titleSize, fontweight='bold')\n",
    "    ax.set_ylim(0, 100)\n",
    "    ax.set_yticks(np.arange(0, 120, 20))\n",
    "    ax.set_yticklabels(['%d%%' % n for n in np.arange(0, 120, 20)], fontsize=ytickSize, fontweight='bold')\n",
    "    ax.set_xticks(indices + 0.5*width)\n",
    "    ax.set_xticklabels(['Top 1', 'Top 5', 'Top 10', 'Top 20'], \n",
    "                       ha='center', fontsize=xtickSize, fontweight='bold')\n",
    "    \n",
    "\n",
    "def plot3Panels():\n",
    "    # 2003 is the first year with >20 trackers. (Checked below)\n",
    "    data3s = percentSitesContaining(TA.sortedEngineYears(engines, firstYear='1996'), \n",
    "                                    TA.sortedEngines(engines, firstYear='1996'), \n",
    "                                    '3')\n",
    "#     dataBs = percentSitesContaining(sortedEngineYears(firstYear='1996'), sortedEngines(firstYear='1996'), 'B')\n",
    "    dataAll = percentSitesContaining(TA.sortedEngineYears(engines, firstYear='1996'), \n",
    "                                     TA.sortedEngines(engines, firstYear='1996'), \n",
    "                                     'ABCDEF')\n",
    "#     liveBs = percentSitesContaining(['Live'], [liveEngine], 'B')\n",
    "    liveAll = percentSitesContaining(['Live'], [liveEngines['Live2016FebNSDI']], 'ABCDEF')\n",
    "\n",
    "    fig, axes = plt.subplots(1,3)\n",
    "    fig.set_tight_layout(True)\n",
    "    fig.set_figwidth(14)\n",
    "    fig.set_figheight(5)\n",
    "    linePlot(data3s, axes[1], 'Third Party', 'upper left', yLabel=False)\n",
    "#     linePlot(dataBs, axes[0], 'Vanilla', 'upper left', yLabel=True)\n",
    "    linePlot(dataAll, axes[0], 'Tracker', 'upper left', yLabel=True)\n",
    "\n",
    "    print(liveAll)\n",
    "#     barPlot(liveBs[0][1:], axes[2])\n",
    "    barPlot(liveAll[0][1:], axes[2])\n",
    "\n",
    "    #plt.savefig('figures/coverage3Panel.png')\n",
    "plot3Panels()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rise and Fall of Historical Champion Trackers (Figure 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Utility functions for champions\n",
    "\n",
    "def sortedEngines(firstYear=None): \n",
    "    if firstYear:\n",
    "        return [e[1] for e in sorted(engines.items()) if e[0] >= firstYear]\n",
    "    else:\n",
    "        return [e[1] for e in sorted(engines.items())]\n",
    "def sortedEngineYears(firstYear=None):\n",
    "    if firstYear:\n",
    "        return [e[0] for e in sorted(engines.items()) if e[0] >= firstYear]\n",
    "    else:\n",
    "        return [e[0] for e in sorted(engines.items())]\n",
    "def getMostPopularTrackersForEngine(engine, typeFilter, topN, year, numChampions=1, insignificance=1):\n",
    "    mongo = pymongo.MongoClient(host=mongoHost, port=mongoPort)\n",
    "    topNSites = set(TF.getOrderedTopNSites(mongo, runNames[year], topN))\n",
    "    instances = trackersByPopularityWithSiteFilter(engine, typeFilter, siteFilter=topNSites)\n",
    "    if len(instances) == 0:\n",
    "        return None\n",
    "    for nonTracker in ['.graphics', '.Graphics', '.pictures', '.pics', '.download', '.media']:\n",
    "        del instances[nonTracker]\n",
    "    champions = instances.most_common(numChampions)\n",
    "    print(engine.year, instances.most_common(3))\n",
    "#     print(year)\n",
    "#     pprint.pprint(instances.most_common())\n",
    "    \n",
    "    trackersToReturn = [x[0] for x in champions if x[1] > insignificance] # Return just names not counts.\n",
    "#     print('returning', trackersToReturn, champions)\n",
    "    return trackersToReturn\n",
    "#     return instances.most_common(1)[0][0]\n",
    "\n",
    "def trackersByPopularityWithSiteFilter(engine, typeFilter, siteFilter=set()):\n",
    "    instances = collections.Counter()\n",
    "#     print('trackers by popularity with %d sites' % len(set(engine.getAllSitesWithTrackers() & siteFilter)))\n",
    "    for site in engine.getAllSitesWithTrackers():\n",
    "        if site not in siteFilter:\n",
    "            continue\n",
    "        for tracker, types in engine.sitesToTrackersToTypes[site].items():\n",
    "            if set(typeFilter) & types:  # Non-empty intersection of filter and types\n",
    "                instances[tracker] += 1\n",
    "    return instances\n",
    "\n",
    "def dedupeWithOrder(seq):\n",
    "    seen = set()\n",
    "    seen_add = seen.add\n",
    "    return [ x for x in seq if not (x in seen or seen_add(x))]\n",
    "\n",
    "@functools.lru_cache(maxsize=6)\n",
    "def getChampionsWithYearlyCoverage(typeFilter, firstYear='1996', topN=500, numChampions=1):\n",
    "    orderedChampions = [getMostPopularTrackersForEngine(engine, typeFilter, topN, year, numChampions=numChampions) \n",
    "                        for engine, year in zip(TA.sortedEngines(engines, firstYear=firstYear), TA.sortedEngineYears(engines, firstYear=firstYear))]\n",
    "    pprint(orderedChampions)\n",
    "    orderedChampions = [champ for sublist in orderedChampions for champ in sublist]\n",
    "    if None in orderedChampions:\n",
    "        orderedChampions.remove(None)\n",
    "    orderedChampions = dedupeWithOrder(orderedChampions)\n",
    "    print('orderedChampions', orderedChampions)\n",
    "    champions = set(orderedChampions)\n",
    "    champions.discard(None)\n",
    "    championToYearlyCoverages = collections.defaultdict(list)\n",
    "    mongo = pymongo.MongoClient(host=mongoHost, port=mongoPort)\n",
    "    for year in TA.sortedEngineYears(engines, firstYear=firstYear):\n",
    "        topNSites = set(TF.getOrderedTopNSites(mongo, runNames[year], topN))\n",
    "#         print('%d topNSites where N is %d' % (len(topNSites), topN))\n",
    "        instances = trackersByPopularityWithSiteFilter(engines[year], typeFilter, siteFilter=topNSites)\n",
    "        totalSites = len(topNSites)\n",
    "        for champ in champions:\n",
    "            if champ in instances:\n",
    "                championToYearlyCoverages[champ].append(instances[champ]/totalSites)\n",
    "            else:\n",
    "                championToYearlyCoverages[champ].append(0)\n",
    "    return championToYearlyCoverages, orderedChampions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# getOrderedTopNSites()\n",
    "def careers(firstYear='1996', topN=500, numChampions=1, typeFilter='3B', ax=None):\n",
    "    def colors():\n",
    "#         someColors = ['#8dd3c7', '#ffffb3', '#bebada', '#fb8072', '#80b1d3', \n",
    "#                       '#fdb462', '#b3de69', '#fccde5', '#d9d9d9', '#bc80bd', '#ccebc5']:\n",
    "#         someColors = [\"#9b59b6\", \"#3498db\", \"#95a5a6\", \"#e74c3c\", \"#34495e\", \"#2ecc71\"]\n",
    "        someColors = [\"windows blue\", \"amber\", \"greyish\", \"faded green\", \"pink\", \"charcoal\"]\n",
    "\n",
    "#         palette = seaborn.color_palette(\"Set\", 16)\n",
    "        palette = seaborn.xkcd_palette(someColors)\n",
    "#         palette[5] = '#663399'\n",
    "#         for color in palette:\n",
    "#             yield color\n",
    "        while True:\n",
    "            for color in ['#000000','#1f78b4','#b2df8a','#e31a1c','#fb9a99','#33a02c',\n",
    "                          '#fdbf6f','#ff7f00','#cab2d6','#6a3d9a','#b15928','#bbbb99',]:\n",
    "                yield color\n",
    "\n",
    "    championToYearlyCoverage, orderedChampions = getChampionsWithYearlyCoverage(typeFilter, firstYear=firstYear, \n",
    "                                                                                topN=topN, numChampions=numChampions)\n",
    "    pprint(orderedChampions)\n",
    "    excludeList = [\n",
    "        'riddler.com', 'ciec.org', 'wired.com', 'hongkong.com', 'akamai.net', 'netguide.com', 'foxworld.com', # manual\n",
    "        'count$wow.net', '.graphics',  # not a URL?\n",
    "        'googleapis.com'  # Only a F> tracker (it sends cookies to enable an F)\n",
    "    ]\n",
    "\n",
    "    championToYearlyCoverage = {k: v for k, v in championToYearlyCoverage.items() if k not in excludeList and k[0] != '.'}\n",
    "\n",
    "    orderedChampions = [c for c in orderedChampions if c not in excludeList and c[0] != '.']\n",
    "    \n",
    "    \n",
    "    \n",
    "    indices = np.arange(len(TA.sortedEngineYears(engines, firstYear=firstYear)))\n",
    "    legendArg = []\n",
    "    print(orderedChampions)\n",
    "    for champ, color in zip(orderedChampions, colors()):\n",
    "        coverages = championToYearlyCoverage[champ]\n",
    "        if champ == 'google-analytics.com':\n",
    "            line = ax.plot(coverages, color=color, linewidth=5, marker='*', markersize=18)\n",
    "        else:\n",
    "            line = ax.plot(coverages, color=color, linewidth=5)\n",
    "        legendArg.append(champ)\n",
    "\n",
    "    ax.legend(legendArg, loc=2, fontsize=legendSize+4)\n",
    "    ax.set_xlabel('Years', fontsize=xlabelSize, fontweight='bold')\n",
    "    ax.set_ylabel('Coverage (of Top 500)', fontsize=ylabelSize, fontweight='bold')\n",
    "    if typeFilter == 'ABCDEF':\n",
    "        trackerName = 'Confirmed Tracker'\n",
    "    elif typeFilter =='3ABCDEF':\n",
    "        trackerName = 'Tracking-Capable or Confirmed'\n",
    "    ax.set_title('Rise And Fall of Historical Champion Trackers', fontsize=titleSize, fontweight='bold')\n",
    "    ax.set_xticks(indices)\n",
    "    ax.set_xticklabels(TA.sortedEngineYears(engines, firstYear=firstYear), \n",
    "                       rotation=45, ha='right', fontsize=xtickSize, fontweight='bold')\n",
    "    ax.set_yticks(np.arange(0, 0.5, 0.05))\n",
    "    ax.set_yticklabels(np.arange(0, 0.5, 0.05), fontsize=xtickSize, fontweight='bold')\n",
    "\n",
    "    #plt.savefig('figures/championCoverage.png', bbox_inches='tight')\n",
    "\n",
    "def bunchaCareers():\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16,12))\n",
    "    careers(firstYear='1996', topN=100, numChampions=1, typeFilter='ABCDEF', ax=axes[0][0])\n",
    "    careers(firstYear='1996', topN=100, numChampions=2, typeFilter='3ABCDEF', ax=axes[0][1])\n",
    "    careers(firstYear='1996', topN=450, numChampions=1, typeFilter='ABCDEF', ax=axes[1][0])\n",
    "    careers(firstYear='1996', topN=450, numChampions=2, typeFilter='3ABCDEF', ax=axes[1][1])\n",
    "\n",
    "def justForPaper():\n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "    careers(firstYear='1996', topN=500, numChampions=2, typeFilter='3ABCDEF', ax=ax)\n",
    "\n",
    "# bunchaCareers()\n",
    "justForPaper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referrer Complexity (Figure 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def graphComplexityTable(graphs):\n",
    "    from tabulate import tabulate\n",
    "    data = []\n",
    "\n",
    "    meanDegrees = []\n",
    "    medianDegrees =[]\n",
    "    withZeroInDegrees = []\n",
    "    withZeroOutDegrees = []\n",
    "    maxInDegrees = []\n",
    "    maxOutDegrees = []\n",
    "\n",
    "    for year, graph in graphs:\n",
    "        inDegrees = [graph.inc_degree(n) for n in graph.node_list()]\n",
    "        outDegrees = [graph.out_degree(n) for n in graph.node_list()]\n",
    "\n",
    "        meanDegrees.append(np.mean(inDegrees))\n",
    "        medianDegrees.append(np.median(inDegrees))\n",
    "        withZeroInDegrees.append(100 * inDegrees.count(0)/len(inDegrees))\n",
    "        withZeroOutDegrees.append(100 * outDegrees.count(0)/len(outDegrees))\n",
    "        maxInDegrees.append(max(inDegrees))\n",
    "        maxOutDegrees.append(max(outDegrees))\n",
    "        \n",
    "        data.append([\n",
    "                year,\n",
    "                '%.2f' % (np.mean(inDegrees)), \n",
    "                '%.1f%%' % (100 * inDegrees.count(0)/len(inDegrees)),\n",
    "                '%.1f%%' % (100 * outDegrees.count(0)/len(outDegrees)),\n",
    "                max(inDegrees),\n",
    "                max(outDegrees),\n",
    "        ])\n",
    "    #print(tabulate(data, tablefmt='latex'))\n",
    "    \n",
    "    fig, ax = plt.subplots(3, 1)\n",
    "    fig.set_figheight(6)\n",
    "    ax[0].plot(meanDegrees, color='black')\n",
    "    ax[0].plot(medianDegrees, marker='o', linestyle='None', color='black')\n",
    "    ax[0].set_ylabel('References', fontsize=TF.ylabelSize-2)\n",
    "    ax[0].legend(('Mean references', 'Median references',), loc='upper left', fontsize=14)\n",
    "    ax[0].set_xticks(range(0, 21, 2))\n",
    "    ax[0].set_xticklabels([])\n",
    "    ax[0].set_yticklabels([str(x) for x in np.arange(0, 3, 0.5)], fontsize=TF.ytickSize)\n",
    "    \n",
    "    ax[1].plot(withZeroInDegrees, linestyle='--', color='black')\n",
    "    ax[1].plot(withZeroOutDegrees, color='black')\n",
    "    ax[1].set_ylabel('Fraction...', fontsize=TF.ylabelSize-2)\n",
    "    ax[1].legend(('... never referred to', '... that refers nothing'), loc='lower left', fontsize=13)\n",
    "    ax[1].set_ylim(0,100)\n",
    "    ax[1].set_xticks(range(0, 21, 2))\n",
    "    ax[1].set_yticklabels(np.arange(0, 1.2, 0.2), fontsize=TF.ytickSize)\n",
    "#     ax[1].get_yaxis().tick_right()\n",
    "\n",
    "    ax[1].set_xticklabels([])\n",
    "    \n",
    "    ax[2].plot(maxInDegrees, linestyle='--', color='black')\n",
    "    ax[2].plot(maxOutDegrees, color='black')\n",
    "    ax[2].set_ylabel('References', fontsize=TF.ylabelSize-2)\n",
    "    ax[2].legend(('Max times referred', 'Max 3rd parties\\nreferred'), loc='upper left', fontsize=14)\n",
    "    ax[2].set_ylim(0,200)\n",
    "    ax[2].set_xticks(range(0, 21, 4))\n",
    "    ax[2].set_xticklabels([str(year) for year in range(1996, 2017, 4)], fontsize=TF.xtickSize)\n",
    "    ax[2].set_yticklabels([str(x) for x in range(0, 250, 50)], fontsize=TF.ytickSize)\n",
    "    \n",
    "    for a in ax:\n",
    "        a.get_yaxis().set_label_coords(-0.125, 0.5)\n",
    "\n",
    "    fig.suptitle('Referer Complexity', fontsize=TF.titleSize, fontweight='bold')\n",
    "    fig.subplots_adjust(bottom=-0.15)\n",
    "    fig.set_figheight(4.5)\n",
    "    #fig.savefig('figures/whaleComplexity.png', bbox_inches='tight')\n",
    "    \n",
    "def averageInDegree(graph):\n",
    "    inDegrees = [graph.inc_degreee(n) for n in graph.node_list()]\n",
    "    \n",
    "    mean = np.mean(inDegrees)\n",
    "    median = np.median(inDegrees)\n",
    "    print('mean: %f' % mean)\n",
    "    print('median: %f' % median)\n",
    "    print('max: %d' % max(inDegrees))\n",
    "    print('0s: %d (%.2f%%)' % (inDegrees.count(0), 100*inDegrees.count(0)/len(inDegrees)))\n",
    "\n",
    "graphComplexityTable([( str(year), graphs[str(year)] ) for year in range(1996, 2017)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
